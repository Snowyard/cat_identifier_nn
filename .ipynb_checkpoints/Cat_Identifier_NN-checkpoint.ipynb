{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train_dataset = h5py.File('train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name='4'></a>\n",
    "## 4 - Two-layer Neural Network\n",
    "\n",
    "<a name='ex-1'></a>\n",
    "\n",
    "\n",
    "Use the helper functions you have implemented in the previous assignment to build a 2-layer neural network with the following structure: *LINEAR -> RELU -> LINEAR -> SIGMOID*. The functions and their inputs are:\n",
    "```python\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    ...\n",
    "    return parameters \n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    ...\n",
    "    return A, cache\n",
    "def compute_cost(AL, Y):\n",
    "    ...\n",
    "    return cost\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    ...\n",
    "    return dA_prev, dW, db\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    ...\n",
    "    return parameters\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z): # relu function returns Activation Value and prior Z value\n",
    "    \n",
    "    A = np.maximum(0, Z)\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def sigmoid(Z): # sigmoid function returns Activation Value and prior Z value\n",
    "    \n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for initilizing parameter shapes for W1, W2, b1, b2 \n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b1 = np.random.randn(n_h, 1)\n",
    "    b2 = np.random.randn(n_y, 1)\n",
    "    \n",
    "    parameters = {\n",
    "        \n",
    "    'W1': W1,\n",
    "    'b1': b1,\n",
    "    'W2': W2,\n",
    "    'b2': b2\n",
    "        \n",
    "    }\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        \n",
    "    cache = (linear_cache, activation_cache)\n",
    "        \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \n",
    "    m = Y.shape[1] \n",
    "    \n",
    "    cost = -(1/m) * np.sum(Y * (np.log(AL)).T + (1 - Y) * (np.log(1 - AL)).T)\n",
    "    cost = np.squeeze(cost)\n",
    "   \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name='6-1'></a>\n",
    "### 6.1 - Linear Backward\n",
    "\n",
    "For layer $l$, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (followed by an activation).\n",
    "\n",
    "Suppose you have already calculated the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. You want to get $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$.\n",
    "\n",
    "The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ are computed using the input $dZ^{[l]}$.\n",
    "\n",
    "Here are the formulas you need:\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n",
    "\n",
    "\n",
    "$A^{[l-1] T}$ is the transpose of $A^{[l-1]}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    \n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = (1/m)*np.dot(dZ, A_prev.T)\n",
    "    db = (1/m)*np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dW, db, dA_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name='6-2'></a>\n",
    "### 6.2 - Linear-Activation Backward\n",
    "\n",
    "Next, you will create a function that merges the two helper functions: **`linear_backward`** and the backward step for the activation **`linear_activation_backward`**. \n",
    "\n",
    "To help you implement `linear_activation_backward`, two backward functions have been provided:\n",
    "- **`sigmoid_backward`**: Implements the backward propagation for SIGMOID unit. You can call it as follows:\n",
    "\n",
    "```python\n",
    "dZ = sigmoid_backward(dA, activation_cache)\n",
    "```\n",
    "\n",
    "- **`relu_backward`**: Implements the backward propagation for RELU unit. You can call it as follows:\n",
    "\n",
    "```python\n",
    "dZ = relu_backward(dA, activation_cache)\n",
    "```\n",
    "\n",
    "If $g(.)$ is the activation function, \n",
    "`sigmoid_backward` and `relu_backward` compute $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}). \\tag{11}$$  \n",
    "\n",
    "<a name='ex-8'></a>\n",
    "### Exercise 8 -  linear_activation_backward\n",
    "\n",
    "Implement the backpropagation for the *LINEAR->ACTIVATION* layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dW, db, dA_prev = linear_backward(dZ, linear_cache) \n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dW, db, dA_prev = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(params, grads, learning_rate = 0.1):\n",
    "    \n",
    "    parameters = copy.deepcopy(params)\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l + 1)] -= learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] -= learning_rate * grads[\"db\" + str(l + 1)]\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of our training set is (12288, 209)\n",
      "The shape of our testing set is (1, 209)\n"
     ]
    }
   ],
   "source": [
    "train_x_orig = (train_x_orig.reshape(train_x_orig.shape[0], -1)).T # flatten to (features, examples m)\n",
    "\n",
    "m = train_x_orig.shape[0] # number of training examples\n",
    "n_x = train_x_orig.shape[1] # number of inputs/features\n",
    "print(\"The shape of our training set is \" + str(train_x_orig.shape))\n",
    "print(\"The shape of our testing set is \" + str(train_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging (Step by Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers: 2\n",
      "\n",
      "\n",
      "Shapes of Params: \n",
      "W1 (4, 12288)\n",
      "b1 (4, 1)\n",
      "W2 (1, 4)\n",
      "b2 (1, 1)\n",
      "\n",
      "\n",
      "X Shape: (12288, 209)\n",
      "Y Shape: (1, 209)\n",
      "\n",
      "\n",
      "A1 Shape: (4, 209)\n",
      "A2 Shape: (1, 209)\n",
      "\n",
      "\n",
      "Cost: 271.45042049379026\n",
      "\n",
      "\n",
      "dA2 Shape: (1, 209)\n",
      "\n",
      "\n",
      "dA1 Shape: (4, 209)\n",
      "dW2 Shape: (1, 4)\n",
      "db2 Shape: (1, 1)\n",
      "\n",
      "\n",
      "dA0 Shape: (4, 209)\n",
      "dW1 Shape: (1, 4)\n",
      "db1 Shape: (1, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,12288) (1,4) (4,12288) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 73\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdb1 Shape: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(db1\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m     66\u001b[0m grads \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     67\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdW1\u001b[39m\u001b[38;5;124m\"\u001b[39m : dW1,\n\u001b[1;32m     68\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdW2\u001b[39m\u001b[38;5;124m\"\u001b[39m : dW2,\n\u001b[1;32m     69\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdb1\u001b[39m\u001b[38;5;124m\"\u001b[39m : db1,\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdb2\u001b[39m\u001b[38;5;124m\"\u001b[39m : db2\n\u001b[1;32m     71\u001b[0m         }\n\u001b[0;32m---> 73\u001b[0m parameters \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 8\u001b[0m, in \u001b[0;36mupdate_params\u001b[0;34m(params, grads, learning_rate)\u001b[0m\n\u001b[1;32m      5\u001b[0m L \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(parameters) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(L):\n\u001b[0;32m----> 8\u001b[0m     parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdW\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m      9\u001b[0m     parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parameters\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,12288) (1,4) (4,12288) "
     ]
    }
   ],
   "source": [
    "(n_x, n_h, n_y) = (12288, 4, 1)\n",
    "parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "\n",
    "# L = len(parameters) // 2\n",
    "print(\"Layers: \" + str(L))\n",
    "print(\"\\n\")\n",
    "\n",
    "# check parameters shapes\n",
    "print(\"Shapes of Params: \")\n",
    "for i in range(0, L):\n",
    "    \n",
    "    print(\"W\" + str(i + 1) + \" \" + str(parameters[\"W\" + str(i + 1)].shape))\n",
    "    print(\"b\" + str(i + 1) + \" \" + str(parameters[\"b\" + str(i + 1)].shape))\n",
    "\n",
    "    \n",
    "print(\"\\n\")\n",
    "    \n",
    "print(\"X Shape: \" + str(X.shape))\n",
    "print(\"Y Shape: \" + str(Y.shape))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "W1 = parameters[\"W1\"]\n",
    "W2 = parameters[\"W2\"]\n",
    "b1 = parameters[\"b1\"]\n",
    "b2 = parameters[\"b2\"]\n",
    "\n",
    "X = train_x_orig\n",
    "Y = train_y\n",
    "\n",
    "# first pass of forward propagation\n",
    "A1, cache1 = linear_activation_forward(X, W1, b1, \"relu\")\n",
    "print(\"A1 Shape: \" + str(A1.shape))\n",
    "\n",
    "# second pass of forward propagation\n",
    "A2, cache2 = linear_activation_forward(A1, W2, b2, \"sigmoid\")\n",
    "print(\"A2 Shape: \" + str(A2.shape))\n",
    "print(\"\\n\")\n",
    "\n",
    "# compute cost\n",
    "cost = compute_cost(A2, Y)\n",
    "print(\"Cost: \" + str(cost))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# find dA2 to find dW's and db's\n",
    "dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "print(\"dA2 Shape: \" + str(dA2.shape))\n",
    "print(\"\\n\")\n",
    "\n",
    "# first pass of back prop\n",
    "dA1, dW2, db2 = linear_activation_backward(dA2, cache2, \"sigmoid\")\n",
    "print(\"dA1 Shape: \" + str(dA1.shape))\n",
    "print(\"dW2 Shape: \" + str(dW2.shape))\n",
    "print(\"db2 Shape: \" + str(db2.shape))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# second pass of back prop\n",
    "dA0, dW1, db1 = linear_activation_backward(dA2, cache2, \"sigmoid\")\n",
    "print(\"dA0 Shape: \" + str(dA0.shape))\n",
    "print(\"dW1 Shape: \" + str(dW1.shape))\n",
    "print(\"db1 Shape: \" + str(db1.shape))\n",
    "\n",
    "grads = {\n",
    "            \"dW1\" : dW1,\n",
    "            \"dW2\" : dW2,\n",
    "            \"db1\" : db1,\n",
    "            \"db2\" : db2\n",
    "        }\n",
    "\n",
    "parameters = update_params(parameters, grads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, number_of_iterations = 100, learning_rate = 0.01):\n",
    "    \n",
    "    cost = ()\n",
    "    \n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    for i in range(number_of_iterations):\n",
    "        \n",
    "        W1 = parameters[\"W1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "    \n",
    "        # forward propagation (returns (A, cache))\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, \"relu\")\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, \"sigmoid\")\n",
    "        \n",
    "        # compute costs\n",
    "        cost = compute_cost(A2, Y)\n",
    "\n",
    "        #compute dA2 with respect to cost function, in order to find parameters, dW, db ...\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        \n",
    "        # backward propagation\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, \"sigmoid\")\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA2, cache2, \"relu\")\n",
    "        \n",
    "        grads = {\n",
    "            \"dW1\" : dW1,\n",
    "            \"dW2\" : dW2,\n",
    "            \"db1\" : db1,\n",
    "            \"db2\" : db2\n",
    "        }\n",
    "        \n",
    "        # update parameters\n",
    "        parameters = update_params(parameters, grads)\n",
    "        \n",
    "        print(cost)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,12288) (1,4) (4,12288) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x_orig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m12288\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 38\u001b[0m, in \u001b[0;36mmodel\u001b[0;34m(X, Y, layers_dims, number_of_iterations, learning_rate)\u001b[0m\n\u001b[1;32m     30\u001b[0m grads \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdW1\u001b[39m\u001b[38;5;124m\"\u001b[39m : dW1,\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdW2\u001b[39m\u001b[38;5;124m\"\u001b[39m : dW2,\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdb1\u001b[39m\u001b[38;5;124m\"\u001b[39m : db1,\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdb2\u001b[39m\u001b[38;5;124m\"\u001b[39m : db2\n\u001b[1;32m     35\u001b[0m }\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# update parameters\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m parameters \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(cost)\n",
      "Cell \u001b[0;32mIn[33], line 8\u001b[0m, in \u001b[0;36mupdate_params\u001b[0;34m(params, grads, learning_rate)\u001b[0m\n\u001b[1;32m      5\u001b[0m L \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(parameters) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(L):\n\u001b[0;32m----> 8\u001b[0m     parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdW\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m      9\u001b[0m     parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m grads[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(l \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parameters\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,12288) (1,4) (4,12288) "
     ]
    }
   ],
   "source": [
    "model(train_x_orig, train_y, (12288, 4, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
